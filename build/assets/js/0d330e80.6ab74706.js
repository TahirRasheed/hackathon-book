"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[16],{8453(e,n,r){r.d(n,{R:()=>s,x:()=>i});var o=r(6540);const t={},a=o.createContext(t);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(a.Provider,{value:n},e.children)}},9208(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-3/compute","title":"Chapter 5: Hardware Compute & Real-Time OS","description":"A humanoid robot must execute multiple tasks in parallel:","source":"@site/docs/module-3/05-compute.md","sourceDirName":"module-3","slug":"/module-3/compute","permalink":"/hackathon-book/docs/module-3/compute","draft":false,"unlisted":false,"editUrl":"https://github.com/anthropics/hackathon-book/tree/main/docs/module-3/05-compute.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Actuators & Power","permalink":"/hackathon-book/docs/module-3/actuators"},"next":{"title":"Chapter 6: ROS 2 Software Stack","permalink":"/hackathon-book/docs/module-3/ros2"}}');var t=r(4848),a=r(8453);const s={},i="Chapter 5: Hardware Compute & Real-Time OS",l={},c=[{value:"CPU: Hard Real-Time Control",id:"cpu-hard-real-time-control",level:2},{value:"GPU: Perception and Planning",id:"gpu-perception-and-planning",level:2},{value:"Edge AI Trade-Off: Onboard vs. Cloud",id:"edge-ai-trade-off-onboard-vs-cloud",level:2},{value:"Compute &amp; Hardware Placement: Edge AI Trade-Offs",id:"compute--hardware-placement-edge-ai-trade-offs",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-5-hardware-compute--real-time-os",children:"Chapter 5: Hardware Compute & Real-Time OS"})}),"\n",(0,t.jsx)(n.p,{children:"A humanoid robot must execute multiple tasks in parallel:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time control loops"})," (200\u20131000 Hz): Reading sensors, computing control torques, commanding actuators. Missing a deadline even once can cause the robot to fall."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"})," (20\u201330 Hz): Running vision CNNs to detect objects, estimate grasping points"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning"})," (10\u201320 Hz): Deciding the next action (move forward, grasp object, etc.)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Logging and monitoring"})," (1\u201310 Hz): Recording data for debugging and analysis"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["These tasks have ",(0,t.jsx)(n.strong,{children:"different real-time requirements"}),":"]}),"\n",(0,t.jsx)(n.h2,{id:"cpu-hard-real-time-control",children:"CPU: Hard Real-Time Control"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"control loop must run at a fixed frequency with predictable latency"}),". For walking at 1 m/s, missing even one control cycle (~10 ms) can cause the robot to stumble. This is ",(0,t.jsx)(n.strong,{children:"hard real-time"}),": missing deadlines causes failure."]}),"\n",(0,t.jsxs)(n.p,{children:["Traditionally, hard real-time requires a ",(0,t.jsx)(n.strong,{children:"Real-Time Operating System (RTOS)"})," like VxWorks or QNX. Modern approaches use ",(0,t.jsx)(n.strong,{children:"Linux RT"})," (a patched version of Linux with real-time scheduling) or ",(0,t.jsx)(n.strong,{children:"ROS 2 on a real-time kernel"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Dedicated CPU cores"})," run the control loop. For Boston Dynamics Atlas, this likely means:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"One or more cores reserved exclusively for the control loop at 200\u2013500 Hz"}),"\n",(0,t.jsx)(n.li,{children:"Core isolation: other tasks (perception, logging) cannot interrupt this core"}),"\n",(0,t.jsx)(n.li,{children:"Latency budget: less than 1 ms from sensor reading to actuator command (Barbalace et al., 2020)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"gpu-perception-and-planning",children:"GPU: Perception and Planning"}),"\n",(0,t.jsxs)(n.p,{children:["Vision processing (CNNs) and motion planning (sampling-based algorithms) are computationally expensive. A ",(0,t.jsx)(n.strong,{children:"GPU (Graphics Processing Unit)"})," accelerates these:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision CNN inference"}),": Detecting objects in 640\xd7480 images. Modern networks (like YOLO or Faster R-CNN) can run at 30 FPS on an onboard GPU (e.g., NVIDIA Jetson AGX)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor fusion"}),": Large-scale EKF with 100+ state variables can be GPU-accelerated"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Motion planning"}),": Sampling-based planners (RRT, PRM) benefit from GPU parallelism"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Boston Dynamics Atlas likely uses an NVIDIA GPU for perception. (Kumar & Prasad, 2023)"}),"\n",(0,t.jsx)(n.h2,{id:"edge-ai-trade-off-onboard-vs-cloud",children:"Edge AI Trade-Off: Onboard vs. Cloud"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Onboard GPU"})," (local compute):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Advantage"}),": Low latency (computation happens on the robot, no network delay)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Disadvantage"}),": High power consumption (~50\u2013100 W for GPU)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use case"}),": Tasks requiring less than 100 ms response time (grasp reaction, collision avoidance)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Cloud GPU"})," (remote compute):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Advantage"}),": Low power on robot, access to large models (GPT-scale language models), easy updates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Disadvantage"}),": High latency (~100\u2013500 ms over WiFi/LTE), connectivity dependent, privacy concerns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use case"}),": Offline tasks (planning a complex manipulation sequence, learning new skills)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Module 5 will explore hybrid approaches: use lightweight models onboard for fast reaction, offload expensive models to the cloud for planning. (Qiu et al., 2024)"}),"\n",(0,t.jsx)(n.h2,{id:"compute--hardware-placement-edge-ai-trade-offs",children:"Compute & Hardware Placement: Edge AI Trade-Offs"}),"\n",(0,t.jsx)(n.p,{children:"The decision of where to run computations (onboard CPU/GPU vs. cloud) has cascading effects:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph OnboardCompute["Onboard Compute (Robot)"]\r\n        CPU["CPU<br/>Hard Real-Time<br/>100-1000 Hz<br/>Power: 5-10W"]\r\n        GPU["GPU<br/>Perception & Fusion<br/>30-100 Hz<br/>Power: 50-100W"]\r\n        LightweightAI["Lightweight AI<br/>(MobileNet, DistilBERT)<br/>Fast Inference<br/>Power: 10-20W"]\r\n    end\r\n\r\n    subgraph CloudCompute["Cloud Compute"]\r\n        CloudGPU["Cloud GPU<br/>Expensive AI Models<br/>(GPT, ResNet-152)<br/>Power: Unlimited"]\r\n        Planning["Complex Planning<br/>Offline Optimization<br/>Power: Unlimited"]\r\n    end\r\n\r\n    subgraph Latency["Latency Impact"]\r\n        OnboardLatency["Onboard Latency<br/>0-5 ms<br/>(Local Processing)"]\r\n        CloudLatency["Cloud Latency<br/>100-500 ms<br/>(Network + Server)"]\r\n    end\r\n\r\n    subgraph BatteryImpact["Battery Impact"]\r\n        OnboardBattery["Onboard Battery<br/>Drain: ~50W<br/>~8 Hours @ 5kWh"]\r\n        CloudBattery["Cloud + WiFi<br/>Drain: ~5W<br/>~40 Hours @ 5kWh"]\r\n    end\r\n\r\n    CPU --\x3e|Ideal For| OnboardLatency\r\n    LightweightAI --\x3e|Inference| OnboardLatency\r\n    GPU --\x3e|Fast Perception| OnboardLatency\r\n\r\n    CloudGPU --\x3e|Inference| CloudLatency\r\n    Planning --\x3e|Optimization| CloudLatency\r\n\r\n    OnboardLatency --\x3e|Enables| OnboardBattery\r\n    CloudLatency --\x3e|Enables| CloudBattery\r\n\r\n    note["Decision Tree:<br/>Latency < 100ms?<br/>\u2192 Use Onboard<br/>Latency > 500ms OK?<br/>\u2192 Use Cloud"]\r\n    GPU -.->|Trade-off| note\r\n    LightweightAI -.->|Trade-off| note\r\n\r\n    style OnboardCompute fill:#c8e6c9\r\n    style CloudCompute fill:#ffcccc\r\n    style Latency fill:#ffe0b2\r\n    style BatteryImpact fill:#e1bee7\r\n    style note fill:#fff9c4\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Diagram 4: Compute & Hardware Placement"})," \u2014 Decision tree for edge AI showing latency vs. power trade-offs. Real-time control loops (5\u201310 ms deadlines) must run onboard; expensive models can offload to cloud if latency tolerance is >500 ms. [SOURCE: Edge AI architecture - FR-008 SC-003]"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.p,{children:["Barbalace, A., Luchetta, A., Schmidt, G., Stitt, L., Phelps, P., & Xenofon, D. (2020). Real-time design based on PREEMPT_RT and timing analysis of collaborative robot control system. In ",(0,t.jsx)(n.em,{children:"Intelligent Robotics and Applications: 14th International Conference, ICIRA 2021"})," (pp. 607\u2013619). Springer. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1007/978-3-030-89098-8_56",children:"https://doi.org/10.1007/978-3-030-89098-8_56"})]}),"\n",(0,t.jsxs)(n.p,{children:["Boston Dynamics. (2023). Atlas Robot Specifications and Documentation. Retrieved from ",(0,t.jsx)(n.a,{href:"https://www.bostondynamics.com/",children:"https://www.bostondynamics.com/"})]}),"\n",(0,t.jsxs)(n.p,{children:["Kumar, K., & Prasad, R. (2023). Object Detection with YOLO Model on NAO Humanoid Robot. In ",(0,t.jsx)(n.em,{children:"Pattern Recognition and Machine Intelligence: 10th International Conference, PReMI 2023"})," (pp. 492\u2013502). Springer. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1007/978-3-031-45170-6_51",children:"https://doi.org/10.1007/978-3-031-45170-6_51"})]}),"\n",(0,t.jsxs)(n.p,{children:["Qiu, Y., Zhang, Y., Huang, Z., Liu, H., & Hu, Y. (2024). Deep Reinforcement Learning for Sim-to-Real Transfer in a Humanoid Robot Barista. ",(0,t.jsx)(n.em,{children:"2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)"}),", 1\u20138. IEEE. ",(0,t.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/10907454/",children:"https://ieeexplore.ieee.org/document/10907454/"})]}),"\n",(0,t.jsxs)(n.p,{children:["Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., & Abbeel, P. (2017). Domain randomization for transferring deep neural networks from simulation to the real world. ",(0,t.jsx)(n.em,{children:"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"}),", 23\u201330. IEEE. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1109/IROS.2017.8202133",children:"https://doi.org/10.1109/IROS.2017.8202133"})]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);