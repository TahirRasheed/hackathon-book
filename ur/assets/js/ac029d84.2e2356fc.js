"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[370],{5779(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-3/sensors","title":"Chapter 3: Sensors & Proprioception","description":"A humanoid robot operates in an uncertain world. To control walking, manipulation, and balance, it must answer these questions continuously:","source":"@site/docs/module-3/03-sensors.md","sourceDirName":"module-3","slug":"/module-3/sensors","permalink":"/hackathon-book/ur/docs/module-3/sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/anthropics/hackathon-book/tree/main/docs/module-3/03-sensors.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Mechanical Structure & Kinematics","permalink":"/hackathon-book/ur/docs/module-3/mechanical"},"next":{"title":"Chapter 4: Actuators & Power","permalink":"/hackathon-book/ur/docs/module-3/actuators"}}');var o=r(4848),t=r(8453);const i={},a="Chapter 3: Sensors & Proprioception",c={},l=[{value:"Vision: Perceiving the Environment",id:"vision-perceiving-the-environment",level:2},{value:"Inertial Measurement Unit (IMU): Vestibular Sensing",id:"inertial-measurement-unit-imu-vestibular-sensing",level:2},{value:"Proprioception: Joint Position and Torque Feedback",id:"proprioception-joint-position-and-torque-feedback",level:2},{value:"Force/Torque Sensors: Interaction Sensing",id:"forcetorque-sensors-interaction-sensing",level:2},{value:"Tactile Sensing: Skin-Level Awareness",id:"tactile-sensing-skin-level-awareness",level:2},{value:"Sensor Fusion: From Signals to State",id:"sensor-fusion-from-signals-to-state",level:2},{value:"Sensor-Actuator Real-Time Loop: From Perception to Action",id:"sensor-actuator-real-time-loop-from-perception-to-action",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-sensors--proprioception",children:"Chapter 3: Sensors & Proprioception"})}),"\n",(0,o.jsx)(n.p,{children:"A humanoid robot operates in an uncertain world. To control walking, manipulation, and balance, it must answer these questions continuously:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Where is my body in space?"})," (orientation, position, velocity)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Where is each joint?"})," (position, velocity, torque)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"What forces am I exerting?"})," (gripper force, foot contact force, joint torque)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"What is in my environment?"})," (obstacles, targets, surfaces)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["The robot answers these questions using sensors. Unlike humans, which have ~20 sensory modalities (proprioception, touch, balance, vision, etc.), humanoid robots typically have 4\u20136 primary sensor types. But the ",(0,o.jsx)(n.em,{children:"fusion"})," of these sensors is what creates situational awareness."]}),"\n",(0,o.jsx)(n.h2,{id:"vision-perceiving-the-environment",children:"Vision: Perceiving the Environment"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"RGB-D Cameras"})," (Kinect-style): Provide both color images and depth (distance to pixels). A humanoid typically mounts:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Head cameras"}),": Forward-facing stereo or RGB-D for object detection, grasping target selection"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gripper cameras"}),": Close-range RGB for fine manipulation (approaching an object, verifying grip)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Boston Dynamics Atlas uses stereo cameras and possibly RGB-D for visual perception. Modern approaches use ",(0,o.jsx)(n.strong,{children:"CNN (Convolutional Neural Networks)"})," running on the onboard GPU to detect objects, estimate grasping points, and plan approach trajectories. (Kumar & Prasad, 2023)"]}),"\n",(0,o.jsxs)(n.p,{children:["Vision updates at roughly ",(0,o.jsx)(n.strong,{children:"30 Hz"})," (30 frames per second). This is slow compared to control loops (typically 200\u20131000 Hz). The challenge: the robot must ",(0,o.jsx)(n.em,{children:"predict"})," where an object will be by the time the gripper reaches it, not just react to the current image."]}),"\n",(0,o.jsx)(n.h2,{id:"inertial-measurement-unit-imu-vestibular-sensing",children:"Inertial Measurement Unit (IMU): Vestibular Sensing"}),"\n",(0,o.jsxs)(n.p,{children:["An ",(0,o.jsx)(n.strong,{children:"IMU"})," contains:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration in 3 axes (including gravity)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity (rotation rate) in 3 axes"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'Together, they form the robot\'s "balance sense" \u2014 equivalent to the human vestibular system. The IMU measures:'}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Orientation"}),": By fusing accelerometer (which feels gravity) and gyroscope (which measures rotation), we can compute the robot's lean angle. This is critical for walking \u2014 the robot must detect if it is tipping and activate stabilizing reactions. (Kim et al., 2016)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Linear acceleration"}),": Distinguishes gravity from body acceleration, allowing the robot to detect ground collisions (foot strike) and sudden perturbations."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["IMUs update at ",(0,o.jsx)(n.strong,{children:"100\u2013200 Hz"}),", fast enough to detect and react to balance disturbances during walking."]}),"\n",(0,o.jsx)(n.h2,{id:"proprioception-joint-position-and-torque-feedback",children:"Proprioception: Joint Position and Torque Feedback"}),"\n",(0,o.jsx)(n.p,{children:"Each joint in Atlas has:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Encoder"}),": Measures joint position (angle) with high precision"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Torque sensor"})," (in Series Elastic Actuators): Measures force transmitted through the joint"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Proprioception tells the robot:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Where is each joint?"})," \u2014 Essential for knowing the robot's configuration without external sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"How much force is the joint exerting?"})," \u2014 Essential for compliant control, force-limited manipulation, and detecting collisions"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"For example, when Atlas grasps an object, the gripper force sensor provides feedback: as the gripper closes, force increases until it reaches a setpoint (say, 50 N), then the controller maintains that force. This prevents the robot from crushing fragile objects or slipping on hard ones. (Pratt & Williamson, 1995)"}),"\n",(0,o.jsxs)(n.p,{children:["Proprioceptive sensors update at ",(0,o.jsx)(n.strong,{children:"200\u20131000 Hz"}),", matching the control loop frequency."]}),"\n",(0,o.jsx)(n.h2,{id:"forcetorque-sensors-interaction-sensing",children:"Force/Torque Sensors: Interaction Sensing"}),"\n",(0,o.jsxs)(n.p,{children:["Beyond joint torque, robots have ",(0,o.jsx)(n.strong,{children:"F/T (force/torque) sensors"})," mounted at:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-effectors (gripper)"}),": 6-axis sensors measuring 3D force and 3D torque"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Foot contact"}),": Simple on/off sensors detecting whether the foot is in contact with the ground"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'During walking, foot contact sensors tell the controller "I am pushing against the ground now" \u2014 enabling the transition from swing phase (leg moving forward in the air) to stance phase (leg supporting the body weight).'}),"\n",(0,o.jsx)(n.p,{children:"During manipulation, end-effector F/T sensors provide compliance: the robot can feel if an object is slipping, adjust its grip, and recover before dropping it."}),"\n",(0,o.jsx)(n.h2,{id:"tactile-sensing-skin-level-awareness",children:"Tactile Sensing: Skin-Level Awareness"}),"\n",(0,o.jsxs)(n.p,{children:["Advanced humanoids like Atlas R3 have ",(0,o.jsx)(n.strong,{children:"tactile sensor networks"})," embedded in the skin \u2014 pressure-sensitive mats that detect contact and distribution of force. This enables:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Collision avoidance"}),": Touching something unexpected triggers immediate defensive reactions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasp stability"}),": Distributed pressure sensors confirm the object is stable in the gripper"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interactive control"}),": The robot can be gently guided by hand (compliance mode) rather than requiring explicit commands"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Tactile sensors are still emerging in commercial humanoids but are critical for safe human-robot interaction. (Tong et al., 2024)"}),"\n",(0,o.jsx)(n.h2,{id:"sensor-fusion-from-signals-to-state",children:"Sensor Fusion: From Signals to State"}),"\n",(0,o.jsxs)(n.p,{children:["Individual sensors provide partial information. ",(0,o.jsx)(n.strong,{children:"Sensor fusion"})," combines them:"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Extended Kalman Filter (EKF)"})," is the standard approach:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prediction step"}),": Use kinematic model to predict where the robot should be (based on previous state and control commands)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Measurement step"}),": Read sensors (IMU, encoders, cameras) and update the prediction"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Example: Walking balance recovery"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"IMU detects robot is tipping forward (accelerometer shows lean)"}),"\n",(0,o.jsx)(n.li,{children:"Encoder and proprioception tell us joint positions"}),"\n",(0,o.jsx)(n.li,{children:'EKF fuses these: "The robot is at a 10\xb0 forward lean with forward velocity 0.5 m/s"'}),"\n",(0,o.jsx)(n.li,{children:'Control algorithm reacts: "Increase hip torque to recover balance"'}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Sensor fusion runs at the control loop frequency (",(0,o.jsx)(n.strong,{children:"200\u20131000 Hz"}),") and must handle ",(0,o.jsx)(n.strong,{children:"asynchronous updates"})," \u2014 vision arrives every 33 ms, IMU every 10 ms, joint encoders every 5 ms. The EKF manages these different update rates. (Hoffman et al., 2024)"]}),"\n",(0,o.jsx)(n.h2,{id:"sensor-actuator-real-time-loop-from-perception-to-action",children:"Sensor-Actuator Real-Time Loop: From Perception to Action"}),"\n",(0,o.jsx)(n.p,{children:"The complete data flow from sensor reading to actuation, with realistic latencies:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:'graph LR\r\n    subgraph T0["T = 0 ms"]\r\n        SenRead["Sensor Read<br/>(IMU, Encoders)<br/>Latency: 1-5 ms"]\r\n    end\r\n\r\n    subgraph T1["T = 1-5 ms"]\r\n        Fusion["Sensor Fusion<br/>(EKF Update)<br/>Latency: 1-3 ms"]\r\n    end\r\n\r\n    subgraph T2["T = 2-8 ms"]\r\n        State["State Estimation<br/>(Robot Position,<br/>Velocity, Orientation)<br/>Latency: 0.5 ms"]\r\n    end\r\n\r\n    subgraph T3["T = 2-10 ms"]\r\n        Control["Control Algorithm<br/>(Compute Joint Torques)<br/>Latency: 2-5 ms"]\r\n    end\r\n\r\n    subgraph T4["T = 4-15 ms"]\r\n        MotorCmd["Motor Commands<br/>(Servo Signals)<br/>Latency: 1-2 ms"]\r\n    end\r\n\r\n    subgraph T5["T = 5-17 ms"]\r\n        ActuatorResp["Actuator Response<br/>(Joint Acceleration)<br/>Latency: 5-10 ms"]\r\n    end\r\n\r\n    subgraph T6["T = 10-27 ms"]\r\n        Feedback["Feedback Sensor Read<br/>(Next Cycle)<br/>Latency: 1-5 ms"]\r\n    end\r\n\r\n    SenRead --\x3e|Vision@30Hz,<br/>IMU@100Hz,<br/>Encoders@200Hz| Fusion\r\n    Fusion --\x3e State\r\n    State --\x3e|Estimated<br/>State| Control\r\n    Control --\x3e MotorCmd\r\n    MotorCmd --\x3e|To Actuators<br/>Hydraulic/Electric| ActuatorResp\r\n    ActuatorResp --\x3e|Physical<br/>Movement| Feedback\r\n    Feedback --\x3e|Closes Loop<br/>5-10 ms Total Latency| SenRead\r\n\r\n    style T0 fill:#e1f5ff\r\n    style T1 fill:#fff3e0\r\n    style T2 fill:#f3e5f5\r\n    style T3 fill:#e8f5e9\r\n    style T4 fill:#fce4ec\r\n    style T5 fill:#ede7f6\r\n    style T6 fill:#e0f2f1\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Diagram 2: Sensor-Actuator Data Flow"})," \u2014 Real-time control loop showing latencies at each stage. For 200 Hz control (5 ms cycle time), this entire sequence must complete in 5 ms. Faster loops (1000 Hz = 1 ms cycle) leave even less margin. [SOURCE: Real-time control architecture - FR-008 SC-003]"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(n.p,{children:["Boston Dynamics. (2023). Atlas Robot Specifications and Documentation. Retrieved from ",(0,o.jsx)(n.a,{href:"https://www.bostondynamics.com/",children:"https://www.bostondynamics.com/"})]}),"\n",(0,o.jsxs)(n.p,{children:["Hoffman, E. M., Laurenzi, A., Muratore, L., Tsagarakis, N. G., & Ajoudani, A. (2024). UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots. ",(0,o.jsx)(n.em,{children:"2024 IEEE International Conference on Robotics and Automation (ICRA)"}),", 16891\u201316897. IEEE. ",(0,o.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/10610951/",children:"https://ieeexplore.ieee.org/document/10610951/"})]}),"\n",(0,o.jsxs)(n.p,{children:["Kim, D., Di Carlo, J., Katz, B., Bledt, G., & Kim, S. (2016). Fusion of force-torque sensors, inertial measurements units and proprioception for a humanoid kinematics-dynamics observation. ",(0,o.jsx)(n.em,{children:"2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)"}),", 714\u2013721. IEEE. ",(0,o.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/7363425/",children:"https://ieeexplore.ieee.org/document/7363425/"})]}),"\n",(0,o.jsxs)(n.p,{children:["Kumar, K., & Prasad, R. (2023). Object Detection with YOLO Model on NAO Humanoid Robot. In ",(0,o.jsx)(n.em,{children:"Pattern Recognition and Machine Intelligence: 10th International Conference, PReMI 2023"})," (pp. 492\u2013502). Springer. ",(0,o.jsx)(n.a,{href:"https://doi.org/10.1007/978-3-031-45170-6_51",children:"https://doi.org/10.1007/978-3-031-45170-6_51"})]}),"\n",(0,o.jsxs)(n.p,{children:["Tong, X., Zhang, H., Sun, Y., Chen, X., Zhang, Y., Yang, C., & Liang, W. (2024). Whole-Body Multi-Contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors. ",(0,o.jsx)(n.em,{children:"IEEE Robotics and Automation Letters"}),", Vol. 9, No. 12, 11234\u201311241. IEEE. ",(0,o.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/10706003",children:"https://ieeexplore.ieee.org/document/10706003"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>i,x:()=>a});var s=r(6540);const o={},t=s.createContext(o);function i(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);