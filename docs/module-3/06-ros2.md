# Chapter 6: ROS 2 Software Stack

ROS 2 (Robot Operating System 2) is the **de facto standard middleware** for humanoid robotics. It provides:
- **Message passing**: Nodes communicate via pub-sub (publishers and subscribers)
- **Real-time support**: DDS (Data Distribution Service) middleware with QoS (Quality of Service) guarantees
- **Tools**: Simulation integration (Gazebo), visualization (RViz), debugging

## Node Architecture for Humanoid Control

A typical ROS 2 humanoid stack has these nodes:

```mermaid
graph TB
    subgraph HardwareInterface["Hardware Interface Layer"]
        SensorDrivers["sensor_drivers<br/>(stereo_proc, imu_driver)<br/>100-1000 Hz"]
        MotorDrivers["motor_drivers<br/>(hydraulic_control, servo_interface)<br/>200-1000 Hz"]
    end

    subgraph PerceptionLayer["Perception & Fusion"]
        SensorFusion["sensor_fusion_node<br/>(EKF state estimation)<br/>200-500 Hz"]
        VisionCNN["vision_perception<br/>(object_detect, grasp_planning)<br/>30 Hz"]
    end

    subgraph ControlLayer["Control Layer"]
        Planner["motion_planner<br/>(trajectory generation)<br/>10-20 Hz"]
        TrajectoryCtrl["trajectory_controller<br/>(joint servoing, balance)<br/>200-1000 Hz"]
    end

    subgraph MonitoringLayer["Monitoring & Safety"]
        Watchdog["watchdog_monitor<br/>(deadline detection)<br/>1000 Hz"]
        SafetyMonitor["safety_monitor<br/>(E-stop, fail-safe)<br/>100 Hz"]
    end

    SensorDrivers -->|Raw: Vision, IMU, Encoders<br/>DDS Topics @ High Freq| SensorFusion
    SensorDrivers -->|Image Stream<br/>30 Hz| VisionCNN
    SensorFusion -->|Estimated State<br/>(position, velocity, orientation)| Planner
    Planner -->|Desired Trajectory| TrajectoryCtrl
    TrajectoryCtrl -->|Joint Torques/Positions| MotorDrivers
    MotorDrivers -->|Actuator Commands| SensorDrivers
    TrajectoryCtrl -->|Heartbeat| Watchdog
    SensorFusion -->|Status| SafetyMonitor
    Watchdog -->|Timeout?| SafetyMonitor
    VisionCNN -->|Detected Objects| Planner

    style HardwareInterface fill:#c8e6c9
    style PerceptionLayer fill:#bbdefb
    style ControlLayer fill:#ffe0b2
    style MonitoringLayer fill:#ffccbc
```

**Diagram 3: ROS 2 Node Architecture** — Layered node graph showing data dependencies and communication frequencies. Nodes with tight deadlines (control loop) run at high frequency with real-time QoS settings; perception nodes run at lower frequency with best-effort QoS. Feedback closes the loop through the Hardware Interface Layer. [SOURCE: ROS 2 real-time patterns - FR-008 SC-003]

**Sensor Drivers** (stereo_image_proc, imu_filter_madgwick):
- Read raw sensor data (camera frames, IMU samples, joint encoders)
- Publish on high-frequency topics (100–1000 Hz)

**Sensor Fusion Node** (state_estimator):
- Subscribes to sensor topics
- Runs EKF to estimate robot state (position, velocity, orientation)
- Publishes fused state at control loop frequency

**Motion Planner** (move_base, traj_planner):
- Runs at lower frequency (10–20 Hz) to decide high-level goals
- Publishes desired trajectory or setpoints

**Trajectory Controller** (joint_trajectory_controller, model_predictive_controller):
- Runs at high frequency (200–1000 Hz)
- Tracks desired trajectory, computes joint torques
- Publishes commands to actuators

**Motor Drivers** (hardware interfaces):
- Lowest level: commands to hydraulic valves or motor controllers
- Reads back actual joint positions and torques

## Real-Time Messaging with DDS

Standard ROS messaging (based on TCP) has unpredictable latency. **DDS (Data Distribution Service)** enables **real-time publish-subscribe**:

**QoS (Quality of Service) settings**:
- **Reliability**: Guaranteed delivery vs. best-effort
- **History**: Keep latest message vs. keep all messages
- **Deadline**: Message must arrive within N milliseconds
- **Durability**: Topic survives node restarts

For the control loop (hard real-time), we set:
- Reliability: Reliable (guaranteed delivery)
- Deadline: less than 10 ms (stricter than publish frequency for margin)
- History: Keep latest only (no buffering to maintain freshness)

For perception (soft real-time), we might use:
- Reliability: Best-effort (skip stale images)
- Deadline: 100 ms (comfortable margin for 30 Hz perception)
- History: Keep latest only

(ROS 2 Documentation, 2024)

## Sensor-Actuator Feedback Loops

A key aspect of real-time control is **closing the loop fast**:

**Walking example**:
1. IMU detects robot is tipping forward (accel = 0.5 g forward)
2. Sensor fusion updates state: lean angle = 8°, forward velocity = 0.3 m/s
3. Control loop reads new state
4. Controller computes: "Increase hip torque by 50 Nm to recover"
5. Motor driver commands hip motors
6. Hydraulic pressure increases, hip joint accelerates
7. Joint encoder shows hip angle changing
8. Next control cycle (10 ms later): IMU detects tipping has slowed, new state is lean = 7°, controller reduces torque
9. Repeat: tight feedback loop stabilizes walking

This entire sequence must complete in **less than 10 ms** (one control cycle) for smooth balance. (Wang et al., 2025)

## Integration with Gazebo (Module 2 Connection)

ROS 2 communicates with Gazebo via `gazebo_ros_control`: the same node architecture runs in both simulation (Gazebo) and reality (real hardware). This enables:
- **Testing control algorithms in simulation** before deploying to real robots
- **Faster development**: iterate 100 times in Gazebo, then validate on real hardware
- **Safe experimentation**: crash the simulated robot freely

Module 2 taught you how to build Gazebo models. Module 3 teaches the hardware constraints those models must respect. Module 4 will teach control algorithms that run in both simulation (via ROS 2 Gazebo bridge) and reality.

---

## References

Barbalace, A., Luchetta, A., Schmidt, G., Stitt, L., Phelps, P., & Xenofon, D. (2020). Real-time design based on PREEMPT_RT and timing analysis of collaborative robot control system. In *Intelligent Robotics and Applications: 14th International Conference, ICIRA 2021* (pp. 607–619). Springer. https://doi.org/10.1007/978-3-030-89098-8_56

Qiu, Y., Zhang, Y., Huang, Z., Liu, H., & Hu, Y. (2024). Deep Reinforcement Learning for Sim-to-Real Transfer in a Humanoid Robot Barista. *2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)*, 1–8. IEEE. https://ieeexplore.ieee.org/document/10907454/

ROS 2 Documentation. (2024). Real-Time Middleware and DDS Configuration. Retrieved from https://docs.ros.org/

Sheridan, T. B., & Parasuraman, R. (2022). Robotic Vision for Human-Robot Interaction and Collaboration: A Survey and Systematic Review. *ACM Transactions on Human-Robot Interaction*, Vol. 12, No. 1, Article 4, 1–66. ACM. https://doi.org/10.1145/3570731

Wang, X., Guo, W., Zhang, T., Lu, Z., & Zhao, M. (2025). Robust Dynamic Walking for Humanoid Robots via Computationally Efficient Footstep Planner and Whole-Body Control. *Journal of Intelligent & Robotic Systems*, Vol. 111, Article 49. Springer. https://doi.org/10.1007/s10846-025-02249-w
