<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-3/sensors" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 3: Sensors &amp; Proprioception | The Robotic Nervous System: ROS 2 for Humanoid AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://tahirrasheed.github.io/hackathon-book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://tahirrasheed.github.io/hackathon-book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://tahirrasheed.github.io/hackathon-book/docs/module-3/sensors"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="ur"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 3: Sensors &amp; Proprioception | The Robotic Nervous System: ROS 2 for Humanoid AI"><meta data-rh="true" name="description" content="A humanoid robot operates in an uncertain world. To control walking, manipulation, and balance, it must answer these questions continuously:"><meta data-rh="true" property="og:description" content="A humanoid robot operates in an uncertain world. To control walking, manipulation, and balance, it must answer these questions continuously:"><link data-rh="true" rel="icon" href="/hackathon-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://tahirrasheed.github.io/hackathon-book/docs/module-3/sensors"><link data-rh="true" rel="alternate" href="https://tahirrasheed.github.io/hackathon-book/docs/module-3/sensors" hreflang="en"><link data-rh="true" rel="alternate" href="https://tahirrasheed.github.io/hackathon-book/ur/docs/module-3/sensors" hreflang="ur"><link data-rh="true" rel="alternate" href="https://tahirrasheed.github.io/hackathon-book/docs/module-3/sensors" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Chapter 3: Sensors & Proprioception","item":"https://tahirrasheed.github.io/hackathon-book/docs/module-3/sensors"}]}</script><link rel="stylesheet" href="/hackathon-book/assets/css/styles.4c493b2a.css">
<script src="/hackathon-book/assets/js/runtime~main.542065df.js" defer="defer"></script>
<script src="/hackathon-book/assets/js/main.e5562e93.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hackathon-book/"><div class="navbar__logo"><img src="/hackathon-book/img/logo.svg" alt="ROS 2 Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hackathon-book/img/logo.svg" alt="ROS 2 Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">ROS 2 Nervous System</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hackathon-book/docs/docs">Modules</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/hackathon-book/docs/module-3/sensors" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/hackathon-book/ur/docs/module-3/sensors" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ur">اردو</a></li></ul></div><a href="https://github.com/anthropics/hackathon-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/hackathon-book/docs/docs"><span title="Home" class="linkLabel_WmDU">Home</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link"><span title="Module 1: The Robotic Nervous System" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-1/"><span title="Module Overview" class="linkLabel_WmDU">Module Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-1/intro"><span title="Chapter 1: Introduction to ROS 2" class="linkLabel_WmDU">Chapter 1: Introduction to ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-1/communication"><span title="Chapter 2: ROS 2 Communication" class="linkLabel_WmDU">Chapter 2: ROS 2 Communication</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-1/python-agents"><span title="Chapter 3: Python AI Agents &amp; URDF" class="linkLabel_WmDU">Chapter 3: Python AI Agents &amp; URDF</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link"><span title="Module 2: The Digital Twin" class="categoryLinkLabel_W154">Module 2: The Digital Twin</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-2/"><span title="Module Overview" class="linkLabel_WmDU">Module Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-2/intro"><span title="Chapter 0: Philosophy &amp; Goals" class="linkLabel_WmDU">Chapter 0: Philosophy &amp; Goals</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-2/gazebo"><span title="Chapter 1: Physics with Gazebo" class="linkLabel_WmDU">Chapter 1: Physics with Gazebo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-2/unity"><span title="Chapter 2: Human-Robot Interaction in Unity" class="linkLabel_WmDU">Chapter 2: Human-Robot Interaction in Unity</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-2/sensors"><span title="Chapter 3: Sensors &amp; Sim-to-Real Gaps" class="linkLabel_WmDU">Chapter 3: Sensors &amp; Sim-to-Real Gaps</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--active"><span title="Module 3: Humanoid Robot Architecture" class="categoryLinkLabel_W154">Module 3: Humanoid Robot Architecture</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/intro"><span title="Chapter 1: Introduction &amp; Learning Objectives" class="linkLabel_WmDU">Chapter 1: Introduction &amp; Learning Objectives</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/mechanical"><span title="Chapter 2: Mechanical Structure &amp; Kinematics" class="linkLabel_WmDU">Chapter 2: Mechanical Structure &amp; Kinematics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hackathon-book/docs/module-3/sensors"><span title="Chapter 3: Sensors &amp; Proprioception" class="linkLabel_WmDU">Chapter 3: Sensors &amp; Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/actuators"><span title="Chapter 4: Actuators &amp; Power" class="linkLabel_WmDU">Chapter 4: Actuators &amp; Power</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/compute"><span title="Chapter 5: Hardware Compute &amp; Real-Time OS" class="linkLabel_WmDU">Chapter 5: Hardware Compute &amp; Real-Time OS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/ros2"><span title="Chapter 6: ROS 2 Software Stack" class="linkLabel_WmDU">Chapter 6: ROS 2 Software Stack</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/safety"><span title="Chapter 7: Safety, Redundancy &amp; Integration" class="linkLabel_WmDU">Chapter 7: Safety, Redundancy &amp; Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/case-study"><span title="Chapter 8: Boston Dynamics Atlas Case Study" class="linkLabel_WmDU">Chapter 8: Boston Dynamics Atlas Case Study</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-book/docs/module-3/summary"><span title="Chapter 9: Summary &amp; Review Questions" class="linkLabel_WmDU">Chapter 9: Summary &amp; Review Questions</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 3: Humanoid Robot Architecture</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 3: Sensors &amp; Proprioception</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 3: Sensors &amp; Proprioception</h1></header>
<p>A humanoid robot operates in an uncertain world. To control walking, manipulation, and balance, it must answer these questions continuously:</p>
<ul>
<li class=""><strong>Where is my body in space?</strong> (orientation, position, velocity)</li>
<li class=""><strong>Where is each joint?</strong> (position, velocity, torque)</li>
<li class=""><strong>What forces am I exerting?</strong> (gripper force, foot contact force, joint torque)</li>
<li class=""><strong>What is in my environment?</strong> (obstacles, targets, surfaces)</li>
</ul>
<p>The robot answers these questions using sensors. Unlike humans, which have ~20 sensory modalities (proprioception, touch, balance, vision, etc.), humanoid robots typically have 4–6 primary sensor types. But the <em>fusion</em> of these sensors is what creates situational awareness.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-perceiving-the-environment">Vision: Perceiving the Environment<a href="#vision-perceiving-the-environment" class="hash-link" aria-label="Direct link to Vision: Perceiving the Environment" title="Direct link to Vision: Perceiving the Environment" translate="no">​</a></h2>
<p><strong>RGB-D Cameras</strong> (Kinect-style): Provide both color images and depth (distance to pixels). A humanoid typically mounts:</p>
<ul>
<li class=""><strong>Head cameras</strong>: Forward-facing stereo or RGB-D for object detection, grasping target selection</li>
<li class=""><strong>Gripper cameras</strong>: Close-range RGB for fine manipulation (approaching an object, verifying grip)</li>
</ul>
<p>Boston Dynamics Atlas uses stereo cameras and possibly RGB-D for visual perception. Modern approaches use <strong>CNN (Convolutional Neural Networks)</strong> running on the onboard GPU to detect objects, estimate grasping points, and plan approach trajectories. (Kumar &amp; Prasad, 2023)</p>
<p>Vision updates at roughly <strong>30 Hz</strong> (30 frames per second). This is slow compared to control loops (typically 200–1000 Hz). The challenge: the robot must <em>predict</em> where an object will be by the time the gripper reaches it, not just react to the current image.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="inertial-measurement-unit-imu-vestibular-sensing">Inertial Measurement Unit (IMU): Vestibular Sensing<a href="#inertial-measurement-unit-imu-vestibular-sensing" class="hash-link" aria-label="Direct link to Inertial Measurement Unit (IMU): Vestibular Sensing" title="Direct link to Inertial Measurement Unit (IMU): Vestibular Sensing" translate="no">​</a></h2>
<p>An <strong>IMU</strong> contains:</p>
<ul>
<li class=""><strong>Accelerometer</strong>: Measures linear acceleration in 3 axes (including gravity)</li>
<li class=""><strong>Gyroscope</strong>: Measures angular velocity (rotation rate) in 3 axes</li>
</ul>
<p>Together, they form the robot&#x27;s &quot;balance sense&quot; — equivalent to the human vestibular system. The IMU measures:</p>
<ul>
<li class=""><strong>Orientation</strong>: By fusing accelerometer (which feels gravity) and gyroscope (which measures rotation), we can compute the robot&#x27;s lean angle. This is critical for walking — the robot must detect if it is tipping and activate stabilizing reactions. (Kim et al., 2016)</li>
<li class=""><strong>Linear acceleration</strong>: Distinguishes gravity from body acceleration, allowing the robot to detect ground collisions (foot strike) and sudden perturbations.</li>
</ul>
<p>IMUs update at <strong>100–200 Hz</strong>, fast enough to detect and react to balance disturbances during walking.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="proprioception-joint-position-and-torque-feedback">Proprioception: Joint Position and Torque Feedback<a href="#proprioception-joint-position-and-torque-feedback" class="hash-link" aria-label="Direct link to Proprioception: Joint Position and Torque Feedback" title="Direct link to Proprioception: Joint Position and Torque Feedback" translate="no">​</a></h2>
<p>Each joint in Atlas has:</p>
<ul>
<li class=""><strong>Encoder</strong>: Measures joint position (angle) with high precision</li>
<li class=""><strong>Torque sensor</strong> (in Series Elastic Actuators): Measures force transmitted through the joint</li>
</ul>
<p>Proprioception tells the robot:</p>
<ul>
<li class=""><strong>Where is each joint?</strong> — Essential for knowing the robot&#x27;s configuration without external sensors</li>
<li class=""><strong>How much force is the joint exerting?</strong> — Essential for compliant control, force-limited manipulation, and detecting collisions</li>
</ul>
<p>For example, when Atlas grasps an object, the gripper force sensor provides feedback: as the gripper closes, force increases until it reaches a setpoint (say, 50 N), then the controller maintains that force. This prevents the robot from crushing fragile objects or slipping on hard ones. (Pratt &amp; Williamson, 1995)</p>
<p>Proprioceptive sensors update at <strong>200–1000 Hz</strong>, matching the control loop frequency.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="forcetorque-sensors-interaction-sensing">Force/Torque Sensors: Interaction Sensing<a href="#forcetorque-sensors-interaction-sensing" class="hash-link" aria-label="Direct link to Force/Torque Sensors: Interaction Sensing" title="Direct link to Force/Torque Sensors: Interaction Sensing" translate="no">​</a></h2>
<p>Beyond joint torque, robots have <strong>F/T (force/torque) sensors</strong> mounted at:</p>
<ul>
<li class=""><strong>End-effectors (gripper)</strong>: 6-axis sensors measuring 3D force and 3D torque</li>
<li class=""><strong>Foot contact</strong>: Simple on/off sensors detecting whether the foot is in contact with the ground</li>
</ul>
<p>During walking, foot contact sensors tell the controller &quot;I am pushing against the ground now&quot; — enabling the transition from swing phase (leg moving forward in the air) to stance phase (leg supporting the body weight).</p>
<p>During manipulation, end-effector F/T sensors provide compliance: the robot can feel if an object is slipping, adjust its grip, and recover before dropping it.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="tactile-sensing-skin-level-awareness">Tactile Sensing: Skin-Level Awareness<a href="#tactile-sensing-skin-level-awareness" class="hash-link" aria-label="Direct link to Tactile Sensing: Skin-Level Awareness" title="Direct link to Tactile Sensing: Skin-Level Awareness" translate="no">​</a></h2>
<p>Advanced humanoids like Atlas R3 have <strong>tactile sensor networks</strong> embedded in the skin — pressure-sensitive mats that detect contact and distribution of force. This enables:</p>
<ul>
<li class=""><strong>Collision avoidance</strong>: Touching something unexpected triggers immediate defensive reactions</li>
<li class=""><strong>Grasp stability</strong>: Distributed pressure sensors confirm the object is stable in the gripper</li>
<li class=""><strong>Interactive control</strong>: The robot can be gently guided by hand (compliance mode) rather than requiring explicit commands</li>
</ul>
<p>Tactile sensors are still emerging in commercial humanoids but are critical for safe human-robot interaction. (Tong et al., 2024)</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-fusion-from-signals-to-state">Sensor Fusion: From Signals to State<a href="#sensor-fusion-from-signals-to-state" class="hash-link" aria-label="Direct link to Sensor Fusion: From Signals to State" title="Direct link to Sensor Fusion: From Signals to State" translate="no">​</a></h2>
<p>Individual sensors provide partial information. <strong>Sensor fusion</strong> combines them:</p>
<p><strong>Extended Kalman Filter (EKF)</strong> is the standard approach:</p>
<ul>
<li class=""><strong>Prediction step</strong>: Use kinematic model to predict where the robot should be (based on previous state and control commands)</li>
<li class=""><strong>Measurement step</strong>: Read sensors (IMU, encoders, cameras) and update the prediction</li>
</ul>
<p>Example: Walking balance recovery</p>
<ul>
<li class="">IMU detects robot is tipping forward (accelerometer shows lean)</li>
<li class="">Encoder and proprioception tell us joint positions</li>
<li class="">EKF fuses these: &quot;The robot is at a 10° forward lean with forward velocity 0.5 m/s&quot;</li>
<li class="">Control algorithm reacts: &quot;Increase hip torque to recover balance&quot;</li>
</ul>
<p>Sensor fusion runs at the control loop frequency (<strong>200–1000 Hz</strong>) and must handle <strong>asynchronous updates</strong> — vision arrives every 33 ms, IMU every 10 ms, joint encoders every 5 ms. The EKF manages these different update rates. (Hoffman et al., 2024)</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-actuator-real-time-loop-from-perception-to-action">Sensor-Actuator Real-Time Loop: From Perception to Action<a href="#sensor-actuator-real-time-loop-from-perception-to-action" class="hash-link" aria-label="Direct link to Sensor-Actuator Real-Time Loop: From Perception to Action" title="Direct link to Sensor-Actuator Real-Time Loop: From Perception to Action" translate="no">​</a></h2>
<p>The complete data flow from sensor reading to actuation, with realistic latencies:</p>
<div class="language-mermaid codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-mermaid codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">graph LR</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph T0[&quot;T = 0 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        SenRead[&quot;Sensor Read&lt;br/&gt;(IMU, Encoders)&lt;br/&gt;Latency: 1-5 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph T1[&quot;T = 1-5 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        Fusion[&quot;Sensor Fusion&lt;br/&gt;(EKF Update)&lt;br/&gt;Latency: 1-3 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph T2[&quot;T = 2-8 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        State[&quot;State Estimation&lt;br/&gt;(Robot Position,&lt;br/&gt;Velocity, Orientation)&lt;br/&gt;Latency: 0.5 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph T3[&quot;T = 2-10 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        Control[&quot;Control Algorithm&lt;br/&gt;(Compute Joint Torques)&lt;br/&gt;Latency: 2-5 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph T4[&quot;T = 4-15 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        MotorCmd[&quot;Motor Commands&lt;br/&gt;(Servo Signals)&lt;br/&gt;Latency: 1-2 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph T5[&quot;T = 5-17 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ActuatorResp[&quot;Actuator Response&lt;br/&gt;(Joint Acceleration)&lt;br/&gt;Latency: 5-10 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph T6[&quot;T = 10-27 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        Feedback[&quot;Feedback Sensor Read&lt;br/&gt;(Next Cycle)&lt;br/&gt;Latency: 1-5 ms&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    SenRead --&gt;|Vision@30Hz,&lt;br/&gt;IMU@100Hz,&lt;br/&gt;Encoders@200Hz| Fusion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Fusion --&gt; State</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    State --&gt;|Estimated&lt;br/&gt;State| Control</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Control --&gt; MotorCmd</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    MotorCmd --&gt;|To Actuators&lt;br/&gt;Hydraulic/Electric| ActuatorResp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ActuatorResp --&gt;|Physical&lt;br/&gt;Movement| Feedback</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Feedback --&gt;|Closes Loop&lt;br/&gt;5-10 ms Total Latency| SenRead</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style T0 fill:#e1f5ff</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style T1 fill:#fff3e0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style T2 fill:#f3e5f5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style T3 fill:#e8f5e9</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style T4 fill:#fce4ec</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style T5 fill:#ede7f6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style T6 fill:#e0f2f1</span><br></span></code></pre></div></div>
<p><strong>Diagram 2: Sensor-Actuator Data Flow</strong> — Real-time control loop showing latencies at each stage. For 200 Hz control (5 ms cycle time), this entire sequence must complete in 5 ms. Faster loops (1000 Hz = 1 ms cycle) leave even less margin. [SOURCE: Real-time control architecture - FR-008 SC-003]</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<p>Boston Dynamics. (2023). Atlas Robot Specifications and Documentation. Retrieved from <a href="https://www.bostondynamics.com/" target="_blank" rel="noopener noreferrer" class="">https://www.bostondynamics.com/</a></p>
<p>Hoffman, E. M., Laurenzi, A., Muratore, L., Tsagarakis, N. G., &amp; Ajoudani, A. (2024). UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots. <em>2024 IEEE International Conference on Robotics and Automation (ICRA)</em>, 16891–16897. IEEE. <a href="https://ieeexplore.ieee.org/document/10610951/" target="_blank" rel="noopener noreferrer" class="">https://ieeexplore.ieee.org/document/10610951/</a></p>
<p>Kim, D., Di Carlo, J., Katz, B., Bledt, G., &amp; Kim, S. (2016). Fusion of force-torque sensors, inertial measurements units and proprioception for a humanoid kinematics-dynamics observation. <em>2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)</em>, 714–721. IEEE. <a href="https://ieeexplore.ieee.org/document/7363425/" target="_blank" rel="noopener noreferrer" class="">https://ieeexplore.ieee.org/document/7363425/</a></p>
<p>Kumar, K., &amp; Prasad, R. (2023). Object Detection with YOLO Model on NAO Humanoid Robot. In <em>Pattern Recognition and Machine Intelligence: 10th International Conference, PReMI 2023</em> (pp. 492–502). Springer. <a href="https://doi.org/10.1007/978-3-031-45170-6_51" target="_blank" rel="noopener noreferrer" class="">https://doi.org/10.1007/978-3-031-45170-6_51</a></p>
<p>Tong, X., Zhang, H., Sun, Y., Chen, X., Zhang, Y., Yang, C., &amp; Liang, W. (2024). Whole-Body Multi-Contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors. <em>IEEE Robotics and Automation Letters</em>, Vol. 9, No. 12, 11234–11241. IEEE. <a href="https://ieeexplore.ieee.org/document/10706003" target="_blank" rel="noopener noreferrer" class="">https://ieeexplore.ieee.org/document/10706003</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/anthropics/hackathon-book/tree/main/docs/module-3/03-sensors.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hackathon-book/docs/module-3/mechanical"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 2: Mechanical Structure &amp; Kinematics</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hackathon-book/docs/module-3/actuators"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 4: Actuators &amp; Power</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#vision-perceiving-the-environment" class="table-of-contents__link toc-highlight">Vision: Perceiving the Environment</a></li><li><a href="#inertial-measurement-unit-imu-vestibular-sensing" class="table-of-contents__link toc-highlight">Inertial Measurement Unit (IMU): Vestibular Sensing</a></li><li><a href="#proprioception-joint-position-and-torque-feedback" class="table-of-contents__link toc-highlight">Proprioception: Joint Position and Torque Feedback</a></li><li><a href="#forcetorque-sensors-interaction-sensing" class="table-of-contents__link toc-highlight">Force/Torque Sensors: Interaction Sensing</a></li><li><a href="#tactile-sensing-skin-level-awareness" class="table-of-contents__link toc-highlight">Tactile Sensing: Skin-Level Awareness</a></li><li><a href="#sensor-fusion-from-signals-to-state" class="table-of-contents__link toc-highlight">Sensor Fusion: From Signals to State</a></li><li><a href="#sensor-actuator-real-time-loop-from-perception-to-action" class="table-of-contents__link toc-highlight">Sensor-Actuator Real-Time Loop: From Perception to Action</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hackathon-book/docs/intro">Module 1</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Official Docs<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/anthropics/hackathon-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Repository<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 AI-Spec-Driven Book Creation. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>